from gqc_agent.core._validations.model_validator import validate_model
from gqc_agent.core.system_prompts.loader import load_system_prompt
from gqc_agent.core._validations.get_supported_models import get_supported_models
from gqc_agent.core._llm_models.gpt_models import (
    gpt_generate,
    GPT_SUPPORTED_MODELS
)

from gqc_agent.core._llm_models.gemini_models import (
    gemini_generate,
    GEMINI_SUPPORTED_MODELS
)


class Client:
    """
    Unified client for interacting with GPT and Gemini LLM models.

    This class handles model validation, system prompts, and automatically routes
    the requests to the correct backend (GPT or Gemini) based on the model type.

    Attributes:
        api_key (str): API key for authenticating with the model service.
        model (str): Name of the model to use.
        system_prompt (str): System prompt content to guide model behavior.
    """
    def __init__(self, api_key: str, model: str, system_prompt: str = "version1.md"):
        """
        Initialize the Client instance.

        Validates the model, loads the system prompt, and stores API credentials.

        Args:
            api_key (str): API key for authentication.
            model (str): Model name (must be in supported GPT or Gemini models).
            system_prompt (str, optional): Filename of the system prompt to load.
                                           Defaults to "version1.md".

        Raises:
            ValueError: If the provided model is not supported.
        """
        self.api_key = api_key
        self.model = model
        self.system_prompt = load_system_prompt(system_prompt)
        validate_model(model)
        
    @classmethod
    def supported_models(cls, models= '__all__'):
        """
        Get a list of supported models.

        This method can return only GPT models, only Gemini models, or all models
        depending on the `models` argument.

        Args:
            models (str, optional): Specify which models to return.
                                    Options: 'gpt', 'gemini', '__all__'.
                                    Defaults to '__all__'.

        Returns:
            list[str]: A list of supported model names.

        Raises:
            ValueError: If an invalid model type is specified (with a suggestion if
                        close match exists).
        """
        return get_supported_models(models)

    def run(self, prompt: str):
        """
        Generate a response from the model based on the given prompt.

        Automatically detects whether the selected model is GPT or Gemini and routes
        the request to the correct backend function. Uses the system prompt to
        guide the model's behavior.

        Args:
            prompt (str): The user input or query to send to the model.

        Returns:
            str: The text response generated by the model.

        Raises:
            ValueError: If the selected model is not supported.
        """

        # ----- GPT MODELS -----
        if self.model in GPT_SUPPORTED_MODELS:
            return gpt_generate(
                api_key=self.api_key,
                model=self.model,
                prompt=prompt,
                system_prompt=self.system_prompt
            )

        # ----- GEMINI MODELS -----
        if self.model in GEMINI_SUPPORTED_MODELS:
            return gemini_generate(
                api_key=self.api_key,
                model=self.model,
                prompt=prompt,
                system_prompt=self.system_prompt
            )

        # If none matched
        raise ValueError(f"Model '{self.model}' not supported.")


