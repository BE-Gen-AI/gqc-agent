{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a18fd56",
   "metadata": {},
   "source": [
    "Example: How to Use the Client to Call a GPT Model\n",
    "\n",
    "This code shows how to use the gqc_agent package to send a prompt to a GPT model.\n",
    "You only need to create a Client object with your API key and model name.\n",
    "After that, you can call .run() to get the model response.\n",
    "\n",
    "How to Use ?\n",
    "\n",
    "1. Import the Client class from the package\n",
    "\n",
    "2. Provide your API key\n",
    "\n",
    "3. Choose the model you want to use: Model name (must be in supported GPT models).\n",
    "\n",
    "4. Load a system prompt file (example(default): \"version1.md\"): Note: The system prompt file should be located in the 'system_prompts' directory.\n",
    "\n",
    "5. Send a user message using client.run()\n",
    "\n",
    "6. Print the LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd548db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT response: {'provider': 'openai', 'model': 'gpt-4o', 'response': \"Hello! I am GQC-Agent, an AI developed to assist with answering questions, providing information, and helping with various tasks. I'm here to offer accurate and concise responses to your queries. How can I assist you today?\"}\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent import Client\n",
    "# Use a valid API key for testing (replace with a real key)\n",
    "OPENAI_API_KEY = \"Your Valid API Key Here\"\n",
    "\n",
    "client_gpt = Client(api_key=OPENAI_API_KEY, model=\"gpt-4o\", system_prompt=\"version1.md\")\n",
    "response = client_gpt.run(\"Hello, can you introduce yourself?\")\n",
    "print(\"GPT response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5692c3b",
   "metadata": {},
   "source": [
    "Example: How to Use the Client to Call a Gemini Model\n",
    "\n",
    "This code shows how to use the gqc_agent package to send a prompt to a Gemini model.\n",
    "You only need to create a Client object with your API key and model name.\n",
    "After that, you can call .run() to get the model response.\n",
    "\n",
    "How to Use ?\n",
    "\n",
    "1. Import the Client class from the package\n",
    "\n",
    "2. Provide your API key\n",
    "\n",
    "3. Choose the model you want to use: Model name (must be in supported Gemini models).\n",
    "\n",
    "4. Load a system prompt file (example(default): \"version1.md\"): Note: The system prompt file should be located in the 'system_prompts' directory.\n",
    "\n",
    "5. Send a user message using client.run()\n",
    "\n",
    "6. Print the LLM response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini response: {'provider': 'gemini', 'model': 'gemini-2.5-flash', 'response': 'Hello! I am GQC-Agent. I am an AI assistant designed to follow instructions strictly and provide accurate and clean responses. How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent import Client\n",
    "# Use a valid API key for testing (replace with a real key)\n",
    "GEMINI_API_KEY = \"Your Valid API Key Here\"\n",
    "\n",
    "client_gemini = Client(api_key=GEMINI_API_KEY, model=\"gemini-2.5-flash\", system_prompt=\"version1.md\")\n",
    "response = client_gemini.run(\"Hello, can you introduce yourself?\")\n",
    "print(\"Gemini response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19faeb6",
   "metadata": {},
   "source": [
    "Example: Using gpt_generate() Directly (Without Client Class)\n",
    "\n",
    "This example shows how to call a GPT model directly using the gpt_generate() function from the package.\n",
    "You provide your API key, select the model, give a prompt, load system prompt and the function returns the response from GPT.\n",
    "\n",
    "Use this when you want full control without using the Client class.\n",
    "\n",
    "How to use:\n",
    "\n",
    "1. Import the gpt_generate function\n",
    "\n",
    "2. Import the load_system_prompt function\n",
    "\n",
    "3. Add your OpenAI API key\n",
    "\n",
    "4. Write the user prompt\n",
    "\n",
    "5. Call the function with:\n",
    "\n",
    " - API key\n",
    "\n",
    " - Model name\n",
    "\n",
    " - Prompt\n",
    "\n",
    " - System prompt file\n",
    "\n",
    "6. Print the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecaf4971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT response: {'provider': 'openai', 'model': 'gpt-4o-mini', 'response': 'Hello! I am GQC-Agent, here to assist you by providing accurate information and answering your questions. How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent.core._llm_models.gpt_models import gpt_generate\n",
    "from gqc_agent.core.system_prompts.loader import load_system_prompt\n",
    "# Use a valid API key for testing (replace with a real key)\n",
    "OPENAI_API_KEY = \"Your Valid API Key Here\"\n",
    "PROMPT = \"Hello, can you introduce yourself?\"\n",
    "SYSTEM_PROMPT = load_system_prompt(\"version1.md\")\n",
    "response = gpt_generate(api_key=OPENAI_API_KEY, model=\"gpt-4o-mini\", prompt=PROMPT, system_prompt=SYSTEM_PROMPT)\n",
    "print(\"GPT response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387af2bb",
   "metadata": {},
   "source": [
    "Example: Using gemini_generate() Directly (Without Client Class)\n",
    "\n",
    "This example shows how to call a Gemini model directly using the gemini_generate() function from the package.\n",
    "You provide your API key, select the model, give a prompt, load system prompt and the function returns the response from Gemini.\n",
    "\n",
    "Use this when you want full control without using the Client class.\n",
    "\n",
    "How to use:\n",
    "\n",
    "1. Import the gemini_generate function\n",
    "\n",
    "2. Import the load_system_prompt function\n",
    "\n",
    "3. Add your Gemini API key\n",
    "\n",
    "4. Write the user prompt\n",
    "\n",
    "5. Call the function with:\n",
    "\n",
    " - API key\n",
    "\n",
    " - Model name\n",
    "\n",
    " - Prompt\n",
    "\n",
    " - System prompt file\n",
    "\n",
    "6. Print the generated response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef4bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini response: {'provider': 'gemini', 'model': 'gemini-2.5-flash', 'response': 'Hello! I am GQC-Agent. I am an AI designed to assist with various tasks, providing accurate and clean responses. How can I help you today?'}\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent.core._llm_models.gemini_models import gemini_generate\n",
    "from gqc_agent.core.system_prompts.loader import load_system_prompt\n",
    "# Use a valid API key for testing (replace with a real key)\n",
    "GEMINI_API_KEY = \"Your Valid API Key Here\"\n",
    "PROMPT = \"Hello, can you introduce yourself?\"\n",
    "SYSTEM_PROMPT = load_system_prompt(\"version1.md\")\n",
    "response = gemini_generate(api_key=GEMINI_API_KEY, model=\"gemini-2.5-flash\", prompt= PROMPT, system_prompt=SYSTEM_PROMPT)\n",
    "print(\"Gemini response:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc344f64",
   "metadata": {},
   "source": [
    "Example: Get List of Supported Gemini Models\n",
    "\n",
    "This example shows how to check which Gemini models are available in the gqc_agent package.\n",
    "You can call the supported_models() class method and pass \"gemini\" to get only Gemini models.\n",
    "\n",
    "Use this when you want to see what models your package supports before making a request.\n",
    "\n",
    "How to use ?\n",
    "\n",
    "1. Import the Client class\n",
    "\n",
    "2. Call Client.supported_models(\"gemini\")\n",
    "\n",
    "3. This returns a list of all supported Gemini model names\n",
    "\n",
    "4. Print the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a6481ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Gemini models: ['gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash']\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent import Client\n",
    "\n",
    "gemini_models = Client.supported_models(\"gemini\")\n",
    "print(\"Supported Gemini models:\", gemini_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d8e9e",
   "metadata": {},
   "source": [
    "Example: Get List of Supported GPT Models\n",
    "\n",
    "This example shows how to check which GPT models are available in the gqc_agent package.\n",
    "You can call the supported_models() class method and pass \"gpt\" to get only GPT models.\n",
    "\n",
    "Use this when you want to see what models your package supports before making a request.\n",
    "\n",
    "How to use ?\n",
    "\n",
    "1. Import the Client class\n",
    "\n",
    "2. Call Client.supported_models(\"gpt\")\n",
    "\n",
    "3. This returns a list of all supported GPT model names\n",
    "\n",
    "4. Print the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875a2013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported GPT models: ['gpt-4o', 'gpt-4o-mini', 'gpt-4.1', 'gpt-4.1-mini']\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent import Client\n",
    "\n",
    "gpt_models = Client.supported_models(\"gpt\")\n",
    "print(\"Supported GPT models:\", gpt_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911bc4e8",
   "metadata": {},
   "source": [
    "Example: Get List of all Supported Models\n",
    "\n",
    "This example shows how to check which models are available in the gqc_agent package.\n",
    "You can call the supported_models() class method and pass \"__all__\" to get only all list of supported models.\n",
    "\n",
    "Use this when you want to see what models your package supports before making a request.\n",
    "\n",
    "How to use ?\n",
    "\n",
    "1. Import the Client class\n",
    "\n",
    "2. Call Client.supported_models(\"__all__\")\n",
    "\n",
    "3. This returns a list of all supported model names\n",
    "\n",
    "4. Print the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583a4a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Supported models: ['gpt-4o', 'gpt-4o-mini', 'gpt-4.1', 'gpt-4.1-mini', 'gemini-2.5-flash', 'gemini-2.5-pro', 'gemini-2.0-flash']\n"
     ]
    }
   ],
   "source": [
    "from gqc_agent import Client\n",
    "\n",
    "supported_models = Client.supported_models(\"__all__\")\n",
    "print(\"All Supported models:\", supported_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
